# -*- coding: utf-8 -*-
"""Maxi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aBlC9XI4cF_keq73gvrC_pAtZiTyYL2v
"""

# Commented out IPython magic to ensure Python compatibility.
#Abro el archivo histórico
import re
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(123456)

#!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth=GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive=GoogleDrive(gauth)

link='https://drive.google.com/open?id=1BtUEu_2I-SXSnmSpjWDqKTMzhwwIU_jh' # The shareable link
fluff, id=link.split('=')
print(id) # Verify that you have everything after '='

downloaded=drive.CreateFile({'id':id}) 
downloaded.GetContentFile('titulo+link.csv')  
dftit=pd.read_csv('titulo+link.csv')
dftit=dftit[dftit['link']!='/']
dftit=dftit.drop(['Unnamed: 0'], axis=1)
dftit=dftit.drop(['num_cat'], axis=1)

# Dataset is now stored in a Pandas Dataframe

#Actualizo el archivo
from bs4 import BeautifulSoup
from urllib.request import urlopen
import pandas as pd
import re

########## ARMO DATAFRAME CON LOS LINKS DE LAS NOTAS PARA GENERAR EL MODELO ##########
link=[]
titulo=[]
categoria=[]
n=list(range(1,10))
topico=['https://buscar.lanacion.com.ar/economia/c-Econom%C3%ADa/page-','https://buscar.lanacion.com.ar/politica/c-Pol%C3%ADtica/page-',
        'https://buscar.lanacion.com.ar/cultura/c-Cultura/page-','https://buscar.lanacion.com.ar/personajes/c-Espect%C3%A1culos/page-',
        'https://buscar.lanacion.com.ar/sociedad/c-Sociedad/page-','https://buscar.lanacion.com.ar/turismo/c-Turismo/page-',
        'https://buscar.lanacion.com.ar/deportes/c-Deportes/page-']

for i in n:
    try:
        for t in topico:
          with urlopen(t+str(i)) as response:
              soup=BeautifulSoup(response,'html.parser')
              h2=soup.find_all('h2')
              title=soup.find_all('title')
              for anchor in h2:            
                titulo.append(anchor.a.text.strip())
                link.append(anchor.a['href'])                
                for anchor in title:                               
                  categoria.append(anchor.text.split('-')[0].strip())        
    except:
        pass  

dicc={'link':link,'titulo':titulo,'categoria':categoria}
dftitnew=pd.DataFrame(dicc)
dftit=pd.concat([dftitnew,dftit])
dftit=dftit[dftit['link']!='/']

#eliminar duplicados
#dftit[dftit['link'].isin(dftit['link'][dftit['link'].duplicated()])]
dftit.drop_duplicates(subset='link', keep='first', inplace=True)

lst_ncat=range(0,len(dftit['categoria'].unique()),1)
lst_cat=dftit['categoria'].unique()
dicc={'num_cat':lst_ncat,'categoria':lst_cat}
dfcat=pd.DataFrame(dicc)
dftit=pd.merge(dftit[['link','titulo','categoria']],dfcat[['categoria','num_cat']],how='left',on='categoria')
dftit

from google.colab import drive
drive.mount('/content/drive')
from google.colab import files
dftit.to_csv('/content/drive/My Drive/PROGRAMACION/TRABAJO FINAL/titulo+link.csv')
#!cp titulo+link.csv "drive/My Drive/"
#files.download('titulo+link.csv')

import nltk
nltk.download('stopwords')
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import spacy

from pathlib import Path
#fileContent = Path(url).read_text()
sw = list(set(stopwords.words('spanish'))) ##### DEFINO UN DICCIONARIO DE STOP WORDS

link='https://drive.google.com/open?id=1BeMeA3GQma9Femlj-2psxinv0B6wgvav' # The shareable link
fluff, id=link.split('=')
print(id) # Verify that you have everything after '='
#downloaded=drive.CreateFile({'id':id}) 
downloaded.GetContentFile('stopwords1.txt')  
f=open("stopwords1.txt", "r")
stopwords1 = list()
with f as f:
  for line in f:
    stopwords1.append(line)
  stopwords1 = [line.rstrip('\n') for line in stopwords1]

def norm_text(text):
  text = text.lower()
  
  # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)
  text=re.sub('\s\W',' ',text)
  text=re.sub('\W\s',' ',text)

  # elimino palabras con 2 letras
  text=re.sub(r'\W*\b\w{1,2}\b', '',text)
  
  # make sure we didn't introduce any double spaces
  text=re.sub('\s+',' ',text)
  
  return text

dftit['tit_norm']=[norm_text(text) for text in dftit['titulo']]
dftit['tit_stpwd']=dftit['tit_norm'].apply(lambda x: " ".join(x for x in x.split() if x not in sw))
dftit['tit_stpwd']=dftit['tit_stpwd'].apply(lambda x: " ".join(x for x in x.split() if x not in stopwords1))

dftit

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score

# pull the data into vectors
vectorizer = CountVectorizer()
x = vectorizer.fit_transform(dftit['tit_stpwd'])

encoder = LabelEncoder()
y = encoder.fit_transform(dftit['categoria'])

# split into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

# take a look at the shape of each of these
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# nb = MultinomialNB()
# nb.fit(x_train, y_train)
# print(nb.score(x_test, y_test))

x_test_pred = nb.predict(x_test)
confusion_matrix(y_test, x_test_pred)

print(classification_report(y_test, x_test_pred, target_names=encoder.classes_))

def make_reverse_vocabulary(vectorizer):
    revvoc = {}

    vocab = vectorizer.vocabulary_
    for w in vocab:
        i = vocab[w]

        revvoc[i] = w

    return revvoc

make_reverse_vocabulary(vectorizer)

voc_freq=make_reverse_vocabulary(vectorizer)
voc_freq=pd.DataFrame(voc_freq.items())
voc_freq.columns=['freq','word'] 
voc_freq=voc_freq.sort_values('freq',ascending=False)
voc_freq.to_csv('/content/drive/My Drive/PROGRAMACION/TRABAJO FINAL/voc_freq.csv')
voc_freq

def predict_cat(title):
    cod=nb.predict(vectorizer.transform([title]))
    return encoder.inverse_transform(cod)[0]

import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

#Links que funcionan:"https://cronista.com/","https://ambito.com/","https://iprofesional.com/"

#Links que NO funcionan:
#"https://lanacion.com.ar/","https://página12.com.ar"


#Rellenar lista con diarios de interés:
diarios=["https://cronista.com/"]
#,"https://ambito.com/","https://iprofesional.com/"]


maestro=[]
for i in diarios:
  page = requests.get(i)
  page.content

  soup=BeautifulSoup(page.content,"html.parser")

  A1=soup.select("a[title]")

  text=[]
  for i in range(len(A1)):
    A0=A1[i].get_text()
    if A0.count(" ")>4:
      text.append(A0)
    else:
      continue

text={'titulo':text}
text=pd.DataFrame(text)
text['tit_norm']=[norm_text(text) for text in text['titulo']]
text['tit_stpwd']=text['tit_norm'].apply(lambda x: " ".join(x for x in x.split() if x not in sw))
text['tit_stpwd']=text['tit_stpwd'].apply(lambda x: " ".join(x for x in x.split() if x not in stopwords1))
text['tit_pred']=[predict_cat(text) for text in text['tit_stpwd']]

from google.colab import files
text.to_csv('/content/drive/My Drive/PROGRAMACION/TRABAJO FINAL/predict_titulo.csv')

text.loc[(text['tit_pred']=='deportes')]

from bs4 import BeautifulSoup
from urllib.request import urlopen
from matplotlib.pyplot import *
import matplotlib.pyplot as plt

corpus = ' '.join(dftit['tit_stpwd'][dftit['categoria']=='deportes'])
corpus
wordcloud = WordCloud(width = 1000, height = 500, background_color="skyblue", colormap="YlOrRd").generate(corpus)
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.savefig("nube"+".png", bbox_inches='tight')
plt.show()
plt.close()

from bs4 import BeautifulSoup
from urllib.request import urlopen
import pandas as pd
import re
from newspaper import fulltext


inter=[]
link=[]
cuerpo=[]
for l in dftit['link'].loc[(dftit['categoria']=='politica')]:    
    try:
      html=requests.get(l).text
      text=fulltext(html)
      link.append(l)
      cuerpo.append(text)
      #with urlopen(l) as response:
        #soup=BeautifulSoup(response,'html.parser')
        #h2=soup.find_all('h2')
        #p=soup.find_all('p')
        #for anchor in h2:            
                #link.append(anchor.a['href'])
                
                #for anchor in p:
                  #inter.append(anchor.text)
                #inter=' '.join([str(item) for item in inter])
                #cuerpo.append(inter)
                #inter=[]                
    except:
        pass

dicc={'link':link,'cuerpo':cuerpo}
dfcuerpo=pd.DataFrame(dicc)

from google.colab import drive
drive.mount('/content/drive')
from google.colab import files
dfcuerpo.to_csv('/content/drive/My Drive/PROGRAMACION/TRABAJO FINAL/cuerpo_politica.csv')
#!cp cuerpo.csv "drive/My Drive/"
#files.download('cuerpo.csv')

#dftit.loc[(dftit['categoria']=='economia')]
dfcuerpo

!pip install newspaper3k
from newspaper import Article

url = 'https://www.lanacion.com.ar/deportes/polo/gonzalo-heguy-a-20-anos-del-repentino-nid2350294'
article = Article(url)
article.download()
article.html
article.parse()

article.authors

article.publish_date

article.text

article.top_image

article.movies

import nltk
nltk.download('punkt')
article.nlp()

article.keywords

article.summary

import newspaper

cnn_paper = newspaper.build('https://www.lanacion.com.ar/')

for article in cnn_paper.articles:
  print(article.url)

for category in cnn_paper.category_urls():
  print(category)

cnn_paper.category_urls

#cnn_article = cnn_paper.articles[0]
article.download()
article.parse()
article.nlp()

from newspaper import fulltext
import requests
html = requests.get(url).text
text = fulltext(html)
url
text